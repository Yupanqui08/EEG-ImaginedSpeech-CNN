{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814a632-a836-4ff3-bcfb-00bbdbad02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 of EEG imagined speech project\n",
    "# Turned in as part of senior project\n",
    "# Used https://www.kaggle.com/vinaykm/eeg-and-dl as starting point\n",
    "# Uses each line of data as input. Results are not above chance (About 6%)\n",
    "# Uses EEG dataset: https://zenodo.org/record/3554128#.Ye85WC-cby8 \n",
    "# April 8, 2022",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization, Conv2D, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "!module load gpu\n",
    "import os\n",
    "print(os.listdir(\"./final_files\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5ed05-5ed4-49c4-b00d-3d0f03d53ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = 1\n",
    "time_steps = 1000\n",
    "subsample = 100\n",
    "#list of csv data files\n",
    "x_paths = [\"training_csvfiles/sub1_data_train.csv\", \"training_csvfiles/sub2_data_train.csv\", \n",
    "           \"training_csvfiles/sub3_data_train.csv\", \"training_csvfiles/sub4_data_train.csv\", \n",
    "           \"training_csvfiles/sub5_data_train.csv\", \"training_csvfiles/sub6_data_train.csv\", \n",
    "           \"training_csvfiles/sub7_data_train.csv\", \"training_csvfiles/sub8_data_train.csv\", \n",
    "           \"training_csvfiles/sub9_data_train.csv\", \"training_csvfiles/sub10_data_train.csv\",\n",
    "           \"training_csvfiles/sub11_data_train.csv\", \"training_csvfiles/sub12_data_train.csv\", \n",
    "           \"training_csvfiles/sub13_data_train.csv\", \"training_csvfiles/sub14_data_train.csv\", \n",
    "           \"training_csvfiles/sub15_data_train.csv\", \"training_csvfiles/sub16_data_train.csv\", \n",
    "           \"training_csvfiles/sub17_data_train.csv\", \"training_csvfiles/sub18_data_train.csv\", \n",
    "           \"training_csvfiles/sub19_data_train.csv\", \"training_csvfiles/sub20_data_train.csv\", \n",
    "           \"training_csvfiles/sub21_data_train.csv\"]\n",
    "\n",
    "#list of csv data files           \n",
    "y_paths = [\"training_csvfiles/sub1_events_train.csv\", \"training_csvfiles/sub2_events_train.csv\", \n",
    "           \"training_csvfiles/sub3_events_train.csv\", \"training_csvfiles/sub4_events_train.csv\", \n",
    "           \"training_csvfiles/sub5_events_train.csv\", \"training_csvfiles/sub6_events_train.csv\", \n",
    "           \"training_csvfiles/sub7_events_train.csv\", \"training_csvfiles/sub8_events_train.csv\", \n",
    "           \"training_csvfiles/sub9_events_train.csv\", \"training_csvfiles/sub10_events_train.csv\",\n",
    "           \"training_csvfiles/sub11_events_train.csv\", \"training_csvfiles/sub12_events_train.csv\", \n",
    "           \"training_csvfiles/sub13_events_train.csv\", \"training_csvfiles/sub14_events_train.csv\", \n",
    "           \"training_csvfiles/sub15_events_train.csv\", \"training_csvfiles/sub16_events_train.csv\", \n",
    "           \"training_csvfiles/sub17_events_train.csv\", \"training_csvfiles/sub18_events_train.csv\", \n",
    "           \"training_csvfiles/sub19_events_train.csv\", \"training_csvfiles/sub20_events_train.csv\",\n",
    "           \"training_csvfiles/sub21_events_train.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04041f-e228-45a8-941c-9bf4d8fdb63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each CSV file in x_paths list, append data to x_data\n",
    "if load:\n",
    "    x_data = []\n",
    "    for x_path in x_paths:\n",
    "        x_path = \"./final_files/\" + x_path\n",
    "        with open(x_path) as file:\n",
    "            for line in file:\n",
    "                if line[0] == \"T\": continue\n",
    "                line_array = []\n",
    "                for word in range(len(line.split(','))-1):\n",
    "                    word+=1\n",
    "                    line_array.append(float(line.split(',')[word]))\n",
    "                line_array = np.asarray(line_array)\n",
    "                x_data.append(line_array)\n",
    "    x_data = np.asarray(x_data)    \n",
    "        \n",
    "# For each CSV file in y_paths list, append data to y_data  \n",
    "    y_data = []\n",
    "    for y_path in y_paths:\n",
    "        y_path = \"./final_files/\" + y_path\n",
    "        with open(y_path) as file:\n",
    "            for line in file:\n",
    "                if line[0] == \"T\": continue\n",
    "                line_array = []\n",
    "                for word in range(len(line.split(','))-1):\n",
    "                    word+=1\n",
    "                    line_array.append(int(line.split(',')[word]))\n",
    "                line_array = np.asarray(line_array)\n",
    "                y_data.append(line_array)\n",
    "    y_data = np.asarray(y_data)\n",
    "\n",
    "print(x_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5e3f3-84b4-4308-8a48-d9f8639b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 28, kernel_size = (7,7), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 14, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = (3,3)))\n",
    "model.add(Conv2D(filters = 28, kernel_size = (5,5), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 14, 1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 28, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 14, 1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters = 28, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 14, 1)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(14, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(16, activation = \"sigmoid\"))\n",
    "\n",
    "adam = Adam(lr = 0.00001)\n",
    "\n",
    "model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def generator(batch_size):\n",
    "    while 1:\n",
    "        \n",
    "        x_time_data = np.zeros((batch_size, time_steps//subsample, 14))\n",
    "        yy = []\n",
    "        for i in range(batch_size):\n",
    "            random_index = np.random.randint(0, len(x_data)-time_steps)\n",
    "            x_time_data[i] = x_data[random_index:random_index+time_steps:subsample]\n",
    "            yy.append(y_data[random_index + time_steps])\n",
    "        yy = np.asarray(yy)\n",
    "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy\n",
    "\n",
    "history=model.fit_generator(generator(14), steps_per_epoch = 9000, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a04628-aa9b-4d7b-863a-33267d2ec55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs showing training accuracies and loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    loss = history.history['loss']\n",
    "    epochs = range(len(acc))\n",
    " \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc',linewidth=2)\n",
    "    plt.title('Training  accuracy')\n",
    "    plt.legend()\n",
    " \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.title('Training loss')\n",
    "    plt.legend()\n",
    " \n",
    "    plt.show()\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52f266-fa07-45b1-a52b-9b8de3dd0e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of csv test data files\n",
    "x_val_paths = [\"testing_csvfiles/sub1_data_test.csv\", \"testing_csvfiles/sub2_data_test.csv\", \n",
    "           \"testing_csvfiles/sub3_data_test.csv\", \"testing_csvfiles/sub4_data_test.csv\", \n",
    "           \"testing_csvfiles/sub5_data_test.csv\", \"testing_csvfiles/sub6_data_test.csv\", \n",
    "           \"testing_csvfiles/sub7_data_test.csv\", \"testing_csvfiles/sub8_data_test.csv\", \n",
    "           \"testing_csvfiles/sub9_data_test.csv\", \"testing_csvfiles/sub10_data_test.csv\",\n",
    "           \"testing_csvfiles/sub11_data_test.csv\", \"testing_csvfiles/sub12_data_test.csv\", \n",
    "           \"testing_csvfiles/sub13_data_test.csv\", \"testing_csvfiles/sub14_data_test.csv\", \n",
    "           \"testing_csvfiles/sub15_data_test.csv\", \"testing_csvfiles/sub16_data_test.csv\", \n",
    "           \"testing_csvfiles/sub17_data_test.csv\", \"testing_csvfiles/sub18_data_test.csv\", \n",
    "           \"testing_csvfiles/sub19_data_test.csv\", \"testing_csvfiles/sub20_data_test.csv\", \n",
    "           \"testing_csvfiles/sub21_data_test.csv\"]\n",
    "\n",
    "#list of csv test event files           \n",
    "y_val_paths = [\"training_csvfiles/sub1_events_train.csv\", \"training_csvfiles/sub2_events_train.csv\", \n",
    "           \"testing_csvfiles/sub3_events_test.csv\", \"testing_csvfiles/sub4_events_test.csv\", \n",
    "           \"testing_csvfiles/sub5_events_test.csv\", \"testing_csvfiles/sub6_events_test.csv\", \n",
    "           \"testing_csvfiles/sub7_events_test.csv\", \"testing_csvfiles/sub8_events_test.csv\", \n",
    "           \"testing_csvfiles/sub9_events_test.csv\", \"testing_csvfiles/sub10_events_test.csv\",\n",
    "           \"testing_csvfiles/sub11_events_test.csv\", \"testing_csvfiles/sub12_events_test.csv\", \n",
    "           \"testing_csvfiles/sub13_events_test.csv\", \"testing_csvfiles/sub14_events_test.csv\", \n",
    "           \"testing_csvfiles/sub15_events_test.csv\", \"testing_csvfiles/sub16_events_test.csv\", \n",
    "           \"testing_csvfiles/sub17_events_test.csv\", \"testing_csvfiles/sub18_events_test.csv\", \n",
    "           \"testing_csvfiles/sub19_events_test.csv\", \"testing_csvfiles/sub20_events_test.csv\",\n",
    "           \"testing_csvfiles/sub21_events_test.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ad814-6718-48ee-835d-ef5155d74dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each CSV file in x_val_paths list, append data to x_val_data\n",
    "x_val_data = []\n",
    "for x_val_path in x_val_paths:\n",
    "    x_val_path = \"./final_files/\" + x_val_path    \n",
    "    with open(x_val_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] == \"T\": continue\n",
    "            line_array = []\n",
    "            for word in range(len(line.split(','))-1):\n",
    "                word+=1\n",
    "                line_array.append(float(line.split(',')[word]))\n",
    "            line_array = np.asarray(line_array)\n",
    "            x_val_data.append(line_array)\n",
    "x_val_data = np.asarray(x_val_data)    \n",
    "\n",
    "# For each CSV file in y_val_paths list, append data to y_val_data\n",
    "y_val_data = []\n",
    "for y_val_path in y_val_paths:\n",
    "    y_val_path = \"./final_files/\" + y_val_path    \n",
    "    with open(y_val_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] == \"T\": continue\n",
    "            line_array = []\n",
    "            for word in range(len(line.split(','))-1):\n",
    "                word+=1\n",
    "                line_array.append(int(line.split(',')[word]))\n",
    "            line_array = np.asarray(line_array)\n",
    "            y_val_data.append(line_array)\n",
    "y_val_data = np.asarray(y_val_data)\n",
    "\n",
    "print(x_val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eecfff0-b2b6-4bcd-9d84-268b7f1e9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data\n",
    "def val_generator():\n",
    "    while 1:\n",
    "        batch_size = 1\n",
    "        x_time_data = np.zeros((batch_size, time_steps//subsample, 14))\n",
    "        yy = []\n",
    "        for i in range(batch_size):\n",
    "            random_index = np.random.randint(0, len(x_val_data)-time_steps)\n",
    "            x_time_data[i] = x_val_data[random_index:random_index+time_steps:subsample]\n",
    "            yy.append(y_val_data[random_index + time_steps])\n",
    "        yy = np.asarray(yy)\n",
    "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee67818-4c86-457a-acb2-6674e3449c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = val_generator()\n",
    "scores = []\n",
    "num_test = 5000\n",
    "for i in range(num_test):\n",
    "    x_test, y_test = next(gen_data)\n",
    "    while not 1 in y_test:\n",
    "        x_test, y_test = next(gen_data)\n",
    "    score=model.evaluate(x_test, y_test, verbose=0)\n",
    "    scores.append(score[1])\n",
    "scores = np.asarray(scores)\n",
    "\n",
    "print(\"Mean \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245a054-25ef-4389-a8f5-a8a2b21d24f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
